%Thread placement and migration
Thread placement can be judiciously managed by runtime systems by monitoring hardware counters and maximizing total local memory accesses across all threads for an OpenMP region~\cite{Su:2011} by factoring in the critical path. Other strategies include examining the communication patterns to discover different thread placement strategies, so that they may benefit from shared caches, through either brute force or heuristic methods~\cite{5581451}. 

%Data placement, migration, and replication
Solaris, Windows and Linux use the first-touch policy by default. To address applications that are not suited for the first-touch policy, that is, where the access patterns are not the same throughout the life of the threads~\cite{Terboven:2008} developed the next-touch policy where the data is marked to be moved to the vicinity (core or node) of the next thread accessing the data. Unfortunately this policy comes with its own set of performance issues and has not been widely accepted for scalability reasons even with improvements~\cite{Goglin:2009} such as kernel based next-touch strategy which migrates only selected pages. For many systems, it may be prudent to replicate data, instead of migrating it. This was demonstrated in~\cite{Bull02} where the cost of replication was less than migration, through they conceded that some combination of replication and migration can achieve comparable performance. Studies in~\cite{Norden:2008} focus on geographical locality for applications with dynamic access patterns and shows that migration can lead to better performance and the need for directive level migration-on-next-touch support for OpenMP applications. Data mapping suggestions and page placement for different architectures has been explored in~\cite{Smeds2003,Marathe:2006}. Up to 20\% improvement in the benchmarks€™s performance was observed~\cite{Marathe:2006} when page placement was directed via feedback about the memory accesses and dynamic memory allocation. Though this study is specific to Itanium-2 general principles are applicable to all ccNUMA systems.

Dynamic thread distribution as studied by~\cite{Matthias:2009} allows multi-level thread scheduler combined with a NUMA-aware memory manager to provide hints by the runtime to be able to either re-distribute threads or migrate data upon next-touch. For providing better ccNUMA locality of data, dynamic distribution of tasks through locality aware queuing software has shown promise~\cite{DBLP:journals2011}.
